---
tags:
  - interview
  - infrastructure
  - kafka
  - message-brokers
---
Источники:
https://habr.com/ru/companies/itsumma/articles/437446/ - гарантии доставки

### Kafka

Источники:
https://www.youtube.com/watch?v=-AZOi3kP9Js&t
https://bigdataschool.ru/blog/kafka-consumer-groups-and-microservices.html
https://bigdataschool.ru/blog/kafka-exactly-once.html

##### Общее

Kafka - распределённый брокер сообщений. Open source. Используется в платформах с требованием высокой производительности, стриминговой аналитики, интеграцией данных из разных источников. Реализована на Java и Scala.

Свойства:
* распределённость
* отказоустойчивость
* высокая доступность
* надёжность и согласованность данных
* высокая производительность (пропускная способность)
* горизонтальное масштабирование
* интегрируемость

Основные сущности Kafka:
* broker
* zookeeper
* message (record)
* topic/partition
* producer
* consumer

##### Broker
Функции брокера:
* приём сообщений
* хранение сообщений
* выдача сообщений

Kafka кластер предоставляет возможности масштабирования и репликации.

##### Zookeeper

* хранит состояние кластера
* хранит конфигурацию
* хранит адресную книгу (data, информация о топиках, партициях)
* обеспечивает выбор Kafka Controller (master node)
* обеспечивает консистентность

##### Message

Сообщение - пара key-value:
* key - ключ (опциональный), используется для распределения сообщений по кластеру
* value - содержимое сообщения, массив байт
* timestamp - время сообщения (от эпохи), устанавливается при отправке или обработке внутри кластера
* headers - набор key-value пар с пользовательскими атрибутами сообщения

##### Topic, partition

**Топик** - поток данных, обычно однотипных; упорядоченная очередь FIFO.
Свойства:
* Данные из топика не удаляются.
* В топике может быть несколько **партиций**. Нужно для ускорения чтения/записи данных (параллелизация).
* Порядок сообщений сохраняется в рамках партиции, а не всего топика.

**Расположение топиков и партиций в брокерах кластера**
Возможна несбалансированность в размещении партиций топика, т.к. учитывается только количество всех партиций всех топиков на брокер.
Можно исправить конфигурацией.

**Хранение данных**
Данные хранятся в log-файлах.
Есть папка `logs`, в которой есть папки на каждую партицию (`A-0`, `A-1`, `A-2` и т.д., где `A` - название топика, числа - номера партиций).
В папках парциций файлы трёх типов:
* `00000000000000000000.log` - хранит сами данные. Колонки:
	* `Offset` - номер сообщения в партиции
	* `Position` - смещение в данном файле в байтах
	* `Timestamp`
	* `Message`
* `00000000000000000000.index` - маппинг с оффсета на позицию. Колонки:
	* `Offset`
	* `Position`
* `00000000000000000000.timeindex` - маппинг с таймстампа на оффсет. Колонки:
	* `Timestamp`
	* `Offset`

Сегменты:
* Последний - активный.
* Имя файла - start offset. Номер в имени файла - первый оффсет, который находится в данном файле. Например, в файле `00000000000000000002.log` первое сообщение имеет оффсет `2`.
* Segment timestamp - max message timestamp.

**Удаление данных**
Операции удаления данных из топика нет.
Поддерживается автоматическое удаление данных по TTL (Time-To-Live):
* удаляются сегменты партиций целиком (не отдельные сообщения);
* segment timestamp expired -> to delete.

**Репликация данных**
Необходима для надёжности и отказоустойчивости - при отказе одного из брокеров данные не будут потеряны.
`replication-factor > 1`
Реплики одной партиции не могут находиться на одном брокере.

В репликации используется master-slave система:
* одна из реплик назначается лидером (leader);
* остальные реплики являются ведомыми (follower);
* лидера назначает Kafka Controller (master broker);
* операции чтения и записи производятся только с leader-репликой;
* возможна проблема несбалансированности leaders и followers - leader-реплики всех партиций находятся на одном брокере, тогда все запросы идут к одному брокеру, остальные отдыхают - можно решить конфигами.

Синхронизация данных между leader и followers:
* followers периодически (с маленьким интервалом) опрашивают leader для получения данных (follower pull leader);
* возможна проблема - данные в репликах могут отставать;
* возможна проблема - при выпадении leader-реплики непонятно, кто из followers должен стать лидером, может оказаться, что ни у кого из followers нет полных данных;
* решение - in-sync replicas (ISR): `min.insync.replicas=3`;
* запись в ISR - синхронная с записью в leader (тормозит запись), в остальных followers - по опросам;
* ISR follower - надёжный кандидат на leader.
Если `replication-factor=4` и `min.insync.replicas=4` и падает один брокер, то становится невозможна запись, поэтому `min.insync.replicas` нужно всегда ставить минимум на 1 меньше количества реплик.

##### Producer

Kafka Producer - высокопроизводительный отправитель сообщений.

У продюсера есть функция send. Параметры функции:
* acks - гарантия доставки:
	* 0 - продюсер не ждёт подтверждения отправки сообщений. Самый ненадёжный режим, могут теряться сообщения.
	* 1 - продюсер ждёт подтверждения отправки сообщений только от leader-реплики. Компромиссный режим, в некоторых случаях могут теряться сообщения - если брокер с leader-репликой упал до реплицирования сообщений.
	* -1 (all) - продюсер ждёт подтверждения отправки сообщений от leader-реплики и всех ISR-реплик. Надёжный режим, сообщения не теряются.
* Семантическая поддержка доставки:
	* at most once;
	* at least once;
	* exactly once - idempotence.

Функция send:
1. fetch metadata:
	* Продюсер через брокер получает из зукипера информацию о состоянии кластера и расположении топика.
	* Дорогая операция! Блокирует send пока получение метаданных не завершится либо до таймаута в 60 секунд.
2. serialize message:
	* Нужно указать key.serializer и value.serializer (например, StringSerializer).
3. define partition:
	* Варианты:
		* explicit partition;
		* round-robin;
		* key-defined (key_hash % n, key_hash считается хеш-функцией MurmurHash2, n - количество партиций).
4. compress message:
	* Используется настройка `compression.codec`.
5. accumulate batch:
	* Сообщение не отправляется сразу, а собирается пакет для повышения производительности;
	* Используются настройки:
		* `batch.size` - до какого размера копим батч;
		* `linger.ms` - таймаут. Если долго копится батч, то он отправляется по таймауту;
	* Если на один брокер формируется два батча и их суммарный объём превышает `batch.size`, то они отправляются.
6. send message.

##### Consumer

Kafka Consumer - высокопроизводительный получатель сообщений.

У продюсера есть функция poll.

Функция poll:
1. fetch metadata:
	* Консюмер через брокер получает из зукипера информацию о состоянии кластера и расположении топика.
2. poll messages:
	* Потребление данных из брокеров (пачками, не по одному сообщению).
	* Подключение к leader-репликам всех партиций топика - в один поток может быть медленно.

**Consumer Group** - группа консюмеров, объединённых в группу (параметр `group.id`).
В группе консюмеров каждый консюмер читает опеределённые партиции.
Если консюмеров больше, чем партиций, какие-то консюмеры будут простаивать.
Если консюмеров меньше, чем партиций, какие-то консюмеры будут обрабатывать по несколько партиций.
Для сбалансированности количество консюмеров должно быть равно количеству партиций.

Kafka Consumer **Offset** - последнее полученное группой консюмеров сообщение.
Есть топик для хранения оффсетов `__consumer_offsets`.
После получения сообщений консюмер коммитит в топик `__consumer_offsets`: `commit A/0/X/2`:
* `Partition` - `A/0` (`A` - топик, `0` - партиция)
* `Group` - `X` (группа консюмеров)
* `Offset` - `2` (прочитал сообщения 0, 1, 2)
При падении одного консюмера из группы, другой, после назначения на него партиции первого, получает из топика `__consumer_offsets` информацию, с какого оффсета он должен начинать читать (`A/0/X/2`).

Типы коммитов:
* Auto commit - at most once (miss messages).
  Возможна проблема - консюмер получает сообщения, сразу коммитит, затем падает, не успев никак обработать сообщения.
* Manual commit - at least once (duplicate messages).
  Возможна проблема - консюмер получает несколько сообщений (т.к. всегда получает пачками), обрабатывает первые несколько из них, затем падает, не успев обработать оставшиеся сообщения и закоммитить. Уже обработанные сообщения обработаются ещё раз, т.к. по ним ещё не было коммита. Если обеспечивается идемпотентность обработки сообщений, проблема неактуальна.
* Custom offset management - exactly once (not missed, no duplicates).
  Нужно реализовать, если требуется политика exactly once. Реализовать хранение оффсетов "у себя" - в БД, файле и т.п. `__consumer_offsets` не используется.

Kafka Consumer Offset missing
* `offsets.retention.minutes` (7 дней) - максимальный период неактивности.
* Если консюмер-группа не подключалась к партиции больше указанного времени, оффсет удаляется из `__consumer_offsets`.
* Когда консюмер-группа поднимется, она не найдёт оффсет.
* `auto.offset.reset` - если не нашли оффсет, используется эта настройка:
	* `earliest` - начинаем читать с самого раннего доступного сообщения;
	* `latest` (default) - не читаем то, что уже есть, читаем только новое.

##### Производительность Kafka

Высокая производительность обусловлена:
* Масштабируемая архитектура
* Последовательные запись и чтение
* Не случайное чтение (no random read) + используется кэширование страниц памяти (последних записанных данных)
* Zero-copy - копирует данные из page memory сразу в socket клиента, в обход приложения (т.к. никаких модификаций данных кафка не производит)
* Огромное количество настроек для разных случаев - для конкретного случая можно очень хорошо затюнить кафку

##### Итого

Kafka:
* очень популярна
* надёжное проверенное решение
* высокопроизводительный инструмент
* широковещательная с упорядочиванием
* интегрируемая со всеми тулами
* гибкая в конфигурации
* нужно понимать нюансы

---

### Rabbit MQ

RabbitMQ - https://www.youtube.com/watch?v=2F_-Lag-_hE

AciveMQ, ArtemisMQ

---

[[!Теория для собеседования]]